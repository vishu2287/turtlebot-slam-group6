\documentclass{ba-kecs}
\usepackage{graphicx,, url}
\usepackage[numbers]{natbib}
\usepackage{amssymb,amsmath}

\title{Turtlebot SLAM }

\author{Maurice Hermans, Lukas Kang, Michael Norget, Thomas Nowicki, Oliver Trinnes}

\begin{document}

\maketitle

\begin{abstract}
This article accompanies the research project "TurtleBot SLAM" from the Master "Artificial Intelligence" of Maastricht University.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Nowadays mobile robots are able to assist industry and society in a diverse set of problems. Transportation, search, rescue and automated vacuum cleaning are examples from different fields. Typically, mobile robots can move through an environment using legs or wheels. In addition they are able to gather information about their environment using sensors. Some even have devices similar to the human arm or hand which allows them to interact with the environment. The vaccuum cleaning robot presented in Figure \ref{fig:vacuum_cleaner} is an example of a robot which has whells to move around. Furthermore it is equipped with a set of sensors. These sensors provide data about the location of obstacles as well as, for example, gaps in the floor the robot is moving on. To efficiently perform this task robots needs a map of its environment. For some tasks map data is available for the robot from the beginning. Provided a map the robot can locate itself using the incoming sensor data. For tasks in unknown environments, however, the robot must rely on internal sensors and make its own map of the environment.

The acquisition of these maps has been a major research focus in the robotics community \cite{Grisetti} [\textbf{MORE SOURCES}]. Building or learning these maps under position uncertainty provided only sensor measurements is often referred to as the Simultaneous Localization And Mapping (SLAM) problem. A large variety of solutions is available to the SLAM problem, these approaches can be classified as either filtering or smoothing [\textbf{SOURCE}].

Filtering approaches model the problem as a so called ``on-line" state estimation problem. The state of such a system consists of the current robot position and the map. New measurements augment and refine the estimate as they become available. Due to their incremental nature these filtering approaches are commonly referred to as on-line SLAM methods. The other type of approach estimates the full trajectory from the full set of measurements. These approaches address the so called full SLAM problem and they typically rely on least-square error minimization techniques [\textbf{SOURCE}].

An intuitive way to formulate the SLAM problem is to consider it as a pose graph optimization problem. The nodes of the graph represent robot poses or landmarks. The edges encode sensor measurements that constraint the connected poses. Once such a graph is constructed the graph-based SLAM method tries to find a configuration of the nodes that is maximally consistent with the contraints contained in the edges. (\textbf{ABSTRACT GRAPH PICTURE HERE})

The rest of this paper is structured as follows. Section \ref{sec:sota} provides the reader with the necessary background information on the topic. Section \ref{sec:impl} is about the implementation details of the methods used. Section \ref{sec:exp} explains the experiments ran to validate and benchmark the implemented methods and also evaluate the obtained results. Section \ref{sec:disc} discusses the acquired results of the experiments and proposes further research based on those. And the last section wraps up the paper with the conclusions of this research.

\begin{figure}[htp]
	\centering
		\includegraphics[width=0.50\textwidth]{figures/vacuum_cleaner.jpg}
	\caption{Automated vacuum cleaner}
	\label{fig:vacuum_cleaner}
\end{figure}

\subsection{Problem Statement}
\label{sec:problem}
This paper accompanies a university group project. The project assignment is to implement both a graph-based SLAM algorithm and an autonomous exploration strategy. The hardware to be used is a mobile robot called TurtleBot. Thus the final product should allow the robot to autonomously navigate through an unknown environment and simultaneously build a map of it using a graph-based SLAM algorithm. Information about SLAM methods, autonomous exploration and the TurtleBot are given in Section \ref{sec:sota}.  

\section{State of the art}
This section presents the state of the art of the hardware, software, and methods used for this project. Subsection \ref{subsec:ros} gives an outline about the operating system in which the product is implemented. Next, Subsection \ref{subsec:turtle} is about the TurtleBot. At last, autonomous exploration (Subsection \ref{subsec:sotaExplore}) and SLAM (Subsection \ref{subsec:sotaSlam}) 
\label{sec:sota} are defined and explained.
\subsection{ROS}
\label{subsec:ros}
The Robot Operating System (ROS) \cite{Quigley} is the environment used throughout the development of all product components for this project. As described by Quigley et al., writing software for robots is difficult since there is a huge variety of possible hardware setups. This also makes code reuse non-trivial. An additional problem is the amount of code needed to acttually ``run" something on a robot. A deep stack including driver-level software, perception modules and, for example, abstract reasoning systems is required before the actual problem can be attacked. Since the necessary expertise and effort is beyond capabilities of most single researchers, an environment that features this functionality is required to achieve anything similar to the problem specified in Subsection \ref{sec:problem}.

To deal with the above mentioned problems robotics researchers have created a wide variety of frameworks. Many robotic software systems are currently used by researchers and industry \cite{Kramer}. Each of these frameworks is designed for one specific purpose. Generally, none of these systems is directly applicable to a problem it was not designed for in the first place.

The ROS framework used in this paper is the product of trade-offs and prioritizations. The emphasis of this framework lies on large-scale integrative robotics research, making it useful in a wide variety of situations. The difference between ROS and the above mentioned frameworks, is ROS' applicability to a wide range of hardware setups.  It provides several so called stacks. Each is an assembly of code made to solve one specific problem. To run a stack or your own code on any robot it is necessary to configure the robot and the ROS framework in a way that they can communicate with each other. For most robots the configuration is straight forward thanks to the online documentation of ROS \citep{Roswiki}.  An example for a stack is the multi-robot simulation environment ``Stage". It is explained in the following paragraph.

\subsubsection{Simulation}
During the development process of a system running on a robot one curcial part is testing. A ROS stack called Stage enables the developers to test code on a simulated robot. Stage allows the simulated robot to move around in virtual environments. It visualizes sensor data and other robot data when required. The user can load her own maps into stage and specify the kind of noise to be applied to measurements. A second important stack needed especially for validation is RViz. It visualizes all requested data. In Figure \ref{fig:stage_and_rviz} Stage and RViz are presented next to each other. While Stage (Left) shows the real map, RViz (Right) presents the map data as seen by the robot.

\subsection{TurtleBot}
 \label{subsec:turtle}

The product of this project is designed to run on a robot called TurtleBot (Figure \ref{fig:turtlebot}). The TurtleBot is a low-cost, customizable, personal robot kit (\textbf{PARAPHRASE?}).The base of the TurtleBot is an iRobot Create (A). It holds a battery pack and 150 degrees/second Single Axis Gyro. The model shown in Figure \ref{fig:turtlebot} uses a Microsoft Kinect sensor (B), the one used in this paper uses a laser range sensor, and a laptop (C) to run all the processes. The robot can also be customized using the mounting hardware (D). The robot runs on an open-source SDK based on ROS which integrates all the software needed to get the TurtleBot running. Already included are some advanced capabilities like mapping and navigation.
\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{figures/turtlebot.png}
	\caption{A TurtleBot}
	\label{fig:turtlebot}
\end{figure}

\subsection{Exploration}
\label{subsec:sotaExplore}
Usually robots localize themselves and navigate through environments using a map which is provided by an external source. In this case navigating from one location to another is an easy problem. If, however, the robot does not have access to a map, navigating through an environment is problematic. The problem treated in this paper (Subsection \ref{sec:problem}) assumes an unknown environment and, hence, no map. The robot needs to be able to explore its environment without any knowledge about it. It needs to rely on the incoming laser and odometry data only. In addition to the creation of the map and the localization of itself (the SLAM problem) the robot needs to explore the environment as efficient as possible. We define a good exploration strategy as one that generates an accurate map in the shortest amount of time.

Ideally the robot updates its internal map of the environment constantly while moving. One idea to efficiently explore the world is to move to locations which maximize the robots information gain. This target location needs to be extracted from the limited information the robot has collected with its sensors.
To achieve this the robot makes use of the idea of frontier-based exploration, first introduced by Yamauchi \cite{Yamauchi} in 1998. Frontiers are defined to be the boundaries between free and unexplored space. Intuitively, when a robot moves to a frontier it is likely that it makes measurements in previously unexplored space. Assuming that the new data is immediately added to its map, the explored region grows once the robot reaches a frontier. Moreover an extended map results in new frontiers. By constantly moving to frontiers the robot expands its knowledge about the environment.

\subsection{SLAM}
\label{subsec:sotaSlam}
As mentioned in Section \ref{sec:intro} Simultaneous Localization And Mapping (SLAM) is the process of building a map based on sensor data while simultaneously localizing yourself in an unknown environment.

SLAM is a problem because, generally, localization requires a map and mapping requires the location of the device or robot that wants to create a map. There are quite a few approaches that successfully solve the SLAM problem. In general there are two interpretations of SLAM. On-line SLAM methods estimate the current robot position along with the map. The core of a system solving the on-line SLAM problem is a filtering technique. FastSlam \citep{Montemerlo02} is an example of an on-line SLAM implementation using a Particle filter to approximate the robots position and the landmark positions on the map. Other methods have the goal to find the trajectory of the robot and the map. They solve the full SLAM problem.   
An example of such a an approach is graph-based SLAM of graphSLAM.

\subsubsection{GraphSLAM}
An intuitive way to formulate the SLAM problem is to view it as a pose graph optimization problem. Each position along the trajectory of the robot is represented by one node. Edges are labelled with relative spatial information between the connecting nodes. This graph is build incrementally. While the robot is in motion, new nodes are added to the graph. While the graph building is a straight forward procedure the essence of the graphSLAM method is the optimization step. In this step the algorithm looks for a configuration of the trajectory which is maximally consistent with a system of constraints obtained from the edges. This problem is commonly solved using least-square error minimization techniques \citep{Grisetti}.

The next section presents the algorithms implemented for this research project.

\section{Implementation}
\label{sec:impl}

The algorithms explained in this sections are implemented in the object-oriented programming language C++. The code is specifically designed for the ROS environment (see Subsection \ref{subsec:ros}). The installation process for ROS, as performed for this paper, can be reviewed on \cite{swarmlab}.\\
(\textbf{TODO:})
For the coding part there exist two main packages, one of which includes the exploration and the other one the mapping part of the SLAM problem. To enable the robot with exploration capabilities, the ROS internal navigation stack for the TurtleBot has been used, making movement possible, by sending a designated goal to the robot.

\subsection{Wavefront frontier detection}

One part of the assignment is autonomous exploration. The robot has to drive autonomously through an unknown world. This feature is implemented using Wavefront frontier detection (WFD) \citep{Keidar}.

WFD is a frontier-based exploration algorithm. The WFD algorithm uses four different data structures. These store map and frontier information during the search process. They are :
\begin{enumerate}
\item{Map-Open-List}
\item{Map-Close-List}
\item{Frontier-Open-List}
\item{Frontier-Close-List}
\end{enumerate}
To understand the need of these four structures it is necessary to know that WFD is using a breadth first search (BFS), starting at the current robot position. Moreover frontier detection requires the robot to have an internal representation of the current environment. In this implementation an Occupancy grid (Subsection \ref{sec:mapping}) is used.
Upon launching the algorithm it first searches for unoccupied space around the current robot location. This grid cells are then saved in the Map-Open-List. 
Frontiers are found by making use of the occupancy grid's structure. The criterion for a map-point to be a frontier is the existence of both, unknown space and known unoccupied space, in adjacent map-points. When a frontier point is found a new BFS is performed. Starting at the current point it extracts all adjacent frontier points and connects them to one frontier. It is saved in the Frontier-Open-List. The benefit of using WFD is that by maintaining these lists, rescans only need to incorporate points that have not been iterated yet. In fact, the algorithm keeps track of detected open space and frontiers. Following iterations make use of this information because they do not need to consider previously known regions any more.
 
Exploration does not only consist of finding frontiers. Based on the frontiers a target location for the robot needs to be selected. Navigating to frontier points directly is the simplest frontier-based exploration strategy. For this project, however, a more sophisticated implementation is chosen. Instead of sending the robot directly to a frontier point, a small search is performed to find suitable destinations. Generally, a small region around the frontier cell is searched. The goal is to have a cell with as much free space around it as possible. For this purposes an evaluation function has been created. It incorporates three features that are used to send a goal to the robot's navigation stack. The three features are :
\begin{enumerate}
\item{Distance of frontier to the robot location}
\item{Number of obstacles around the designated} \item{Distance to the previous goal}
\end{enumerate}
 The features have equal weight. Their sum is used to find the final goal to be sent to the navigation stack. This evaluation function enables the algorithm to select a reachable destination for the robot in almost every case. Additionally the information gain is still next to optimal since the selected goal is preferred to be close to frontier points. At last the third feature makes sure that the robots path is smooth. Selecting a goal near the previous one makes the robot avoid to many loops in the emerging graph.
\subsection{GraphSLAM}
\label{sec:implSLAM}
Two different approaches of graph-based SLAM are implemented for this project. They differ in complexity and thereby applicability to the problem. Both algorithms create an occupancy grid map. 
\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{figures/graph.png}
	\caption{An example of a simple graph (based on \citep{Grisetti}).}
	\label{fig:turtlebot}
\end{figure}
First, an algorithm as suggested by \citep{Grisetti} is implemented. Here the graph consists of nodes representing positions along the robot trajectory. Odometry and sensor measurements are incorporated into the constraints contained in the edges. Similar to the graphSLAM procedure explained in Section \ref{subsec:sotaSlam}, an edge connects two adjacent poses of the robot. This implementation optimizes the pose graph according to the constraints. This ``optimized" trajectory then determines the positions of obstacles on the map by refining the laser scan data according to the poses they were obtained at. Each edge thus is labelled with the spatial difference between two poses \(x_i\) and \(x_j\). Let this error be denoted by \(e_{ij}(x_i, x_j)\). This information is obtained from a combination of odometry data and laser range sensor data. The odometry is corrected using a laser scan matcher (see Subsection \ref{sec:scan}). 

This graph-based SLAM approach calculates the most likely path as follows:
First it finds the Jacobians \(A\) and \(B\) of every two estimated positions connected by an edge.
\begin{equation}
\label{eq:jacobi_a}
	A_{ij} = \frac{\partial e_{ij}(x)}{\partial x_i}\uplus
\end{equation}
\begin{equation}
\label{eq:jacobi_b}
	B_{ij} = \frac{\partial e_{ij}(x)}{\partial x_j}
\end{equation}
Note that in Equations \ref{eq:jacobi_a} and \ref{eq:jacobi_b} the x represents the current guess of the concerning robot position. As explained above, \(e_{ij}(x_i, x_j)\) is the error between the two poses \(x_i\) and \(x_j\). The Jacobians \(A\) and \(B\) are then added to an information matrix \(H\) (Equation \ref{eq:H}).
\begin{equation}
\label{eq:H}
\begin{aligned}
	H_{[ii]} += A^{T}_{ij} \Omega_{ij} A_{ij} \\
	H_{[ij]} += A^{T}_{ij} \Omega_{ij} B_{ij} \\
	H_{[ji]} += B^{T}_{ij} \Omega_{ij} A_{ij} \\
	H_{[jj]} += B^{T}_{ij} \Omega_{ij} B_{ij} \\
\end{aligned}
\end{equation}
In this equation $\Omega_{ij}$ is the information matrix of a measurement between \(x_i\) and \(x_j\).
\\
Furthermore a coefficient vector \(b\) is always updated as shown in Equation \ref{eq:b}. This vector in combination with the \(H\) matrix form a set of linear constraints.
\begin{equation}
\label{eq:b}
\begin{aligned}
	b_{[i]} += A^{T}_{ij} \Omega_{ij} e_{ij} \\
	b_{[j]} += B^{T}_{ij} \Omega_{ij} e_{ij} 
\end{aligned}
\end{equation}
The optimization of the vector \(x = (x_1, ..., x_n)^T \) according to the constraints specified in \(H\) and \(b\) is the core of this approach. This step is performed using sparse Cholesky factorization. 
A new node is added to the graph once the robot moves 0.5 meters. The algorithm optimizes the trajectory like explained once a new node is added.\\
In one special case the algorithm adds an extra edge, connecting to non-adjacent nodes, to the graph. This happens when the distance between the two connected robot positions is smaller than a margin. In fact, this special case enables the algorithm to create loops in the graph and more accurately estimate the robots trajectory and the map. This procedure is referred to as ``loop closing". 
\\
\\
The second implementation is based on \cite{Thrun}. It creates nodes for each odometry measurement of the robot as well as for each feature in the map spotted by the laser sensor. Each occupancy grid cell containing an obstacle is considered a map feature. This is because the map features are all that is added to the occupancy grid in the end. Edges are labelled with spatial constraints of the two nodes they are connecting. The output of this implementation is a vector containing the positions of the land marks as well as the trajectory of the robot. Here the constraints labelling the edges are created using Jacobians similar to the previous approach. The major difference lies in the node creation. Here nodes are added for landmarks while the previous implementation considered the robots' trajectory only. An additional difference lies in the fact that this implementation uses features to characterize the map. The algorithm needs to align each new observed feature with the set of features already seen. This matching step has to be performed and repeated along with the complete optimization procedure until convergence.

\subsection{Scan matching}
\label{sec:scan}
The scan matcher, used by this program, is necessary to acquire an optimized movement vector between two robot positions. This is needed by the GraphSLAM algorithm (see Subsection \ref{sec:implSLAM}). The scan matcher realized for this project can be divided into two steps. First, the two scans are matched making use of raw odometry data, retrieved by the robot's motion sensor. This is improved upon by running an iteration of the iterative closest point algorithm (ICP). It tries to align the two scans minimizing the total distance of each point in the first scan to its nearest neighbour in the second. 

The general procedure of the algorithm is to first find the nearest neighbouring point in the second scan for all point in the first. To achieve this nearest neighbour search is used.
In this implementation multiple points can have the same corresponding neighbour-point. If there is no point from the other scan within a specified radius of one point, it is excluded. These outliers would influence further calculations negatively.
The next step in the process is to calculate the centroid of both clouds. These are subtracted from every point in the concerning cloud. In that way the clouds are centred at the origin. The calculated points are then added to two separate $2xn$ matrices, where $n$ is the number of neighbour pairs. Next the rotation matrix which transforms one cloud to the other is needed. The Singular Value Decomposition (SVD) of the product of the two previously mentioned matrices returns three new matrices $U$, $\Sigma$ and $V$. The required rotation matrix R can be obtained as described in formula \ref{eq:RVUT}.
\begin{equation}
\label{eq:RVUT}
 R = V * U^T
\end{equation}
The translation vector $t$ is the difference between the two centroids. $R$ and $t$ together form a rigid transformation. This transformation matches the two scans. Moreover it is used to correct the robot's odometry data.

\subsection{Mapping}\label{sec:mapping}

The map is internally represented as an occupancy grid. It consists of cells which hold information about obstacles, free/- and unknown space. Using ROS's visualization program, RViz, the map can be visualized on the screen.

The occupancy grid is a structure that is represented by a one-dimensional array. The size of an occupancy grid has to be tuned problem specific. A too large grid can lead to performance problems. If the grid is too small, however, the map will be inaccurate. Fitting a map on a smaller grid results in a low resolution.
Internally the program represents the grid as a two dimensional array. This makes navigating to either side more intuitive. Each grid cell contains an integer value. Different values represent the states a cell can be in. The value-range for ROS's occupancy grid lies between $-1$ and $100$ \cite{occupancy}. Each value has a specific meaning:
\begin{description}
\item{$-1$} : Unknown space, not yet covered by robot's perception sensors
\item{$0$} : Known free space, robot can drive on it safely
\item{$1-100$} : Probability of being an obstacle, $1$ being a low probability and $100$ being a definite obstacle.
\end{description}
\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.45\textwidth]{figures/Occup.png}
	\caption{A visualized occupancy grid.}
	\label{fig:Occupancy}
\end{figure}
Figure \ref{fig:Occupancy} shows an example occupancy grid. The robot is standing in the middle. It senses the state of all cells within a distance of one. The border cells are marked with a $-1$, which means that they are still unexplored. The remaining cells are explored being either an obstacle or free space.
The occupancy grid is created using laser scan data. Each laser scan contains information about whether or not an obstacle has been detected at a certain bearing. Maximum range readings occur when the robot senses only free space in a certain direction. All laser scans are transformed into point clouds. The absolute locations of the points are dependent on the position at which the laser scan was recorded.
In the occupancy grid detected obstacle points are indicated by value 100.
Free space is being incorporated into the map by filling regions between detected points and the concerning robot position. This is achieved with the help of a linear function. 

As time progresses, the robot explores more of its environment. As explained above, the locations of the observed obstacles need to be extracted from the laser-scan data. The program keeps track of the scan-data for every robot position saved in the trajectory \(x = (x_1, ..., x_n)^T \). Since the laser-scan data is relative to the position of recording, new information is added to the map once the robot's trajectory is adjusted by the graphSLAM module. When a new scan is made, it is added to the occupancy grid after the trajectory has been updated. When a loop closure is detected, the complete occupancy grid is updated. All previous scan points are refined relative to their corresponding recording positions and the grid is regenerated.

\section{Experiments and Results}
\label{sec:exp}
This section demonstrates the results of the Turtlebot SLAM implementation as described in section \ref{sec:impl}. Several outputs of the visualization tool RViz are presented and discussed. Comparisons are drawn between different variations of the implementation and the performance of the program is evaluated.

\subsection{Visualization}
When running the program via the launch file, RViz and Stage are automatically started. Stage shows the robot in its actual position on an accurate test map (see figure \ref{fig:stage_and_rviz} left). RViz shows the current belief of the robot about its environment, position and trajectory (see figure \ref{fig:stage_and_rviz} right).

The components of the pose graph and the environment are highlighted by the following labels:

\begin{description}
\item[green dots: ]past positions of the robot, saved as nodes of the pose graph
\item[red arrows: ]past orientations of the robot, included in the nodes of the pose graph
\item[blue lines: ]measurement constraints between the connected poses
\item[yellow dots: ]frontier points detected by the wavefront frontier detection algorithm
\item[orange dot: ] next navigation goal
\item[black area: ] detected obstacles
\item[light gray area: ] detected free space
\item[dark gray area: ] unexplored space
\end{description}

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.50\textwidth]{figures/Stage_and_rviz.jpg}
	\caption{Robot and environment visualization with Stage (left) and RViz (right)}
	\label{fig:stage_and_rviz}
\end{figure}

\subsection{Loop closing effect}
One of the core properties of the Graph SLAM algorithm is the correction of the pose graph and the map whenever a loop closing is detected. This means that when the robot has been exploring an unknown area and then reenters an explored area, the transformation between the current pose and the past re-detected pose gives essential information on the complete pose graph, correcting the current belief about the environment and position.

The effect of loop closing detection is illustrated in figure \ref{fig:loop_closing_comparison}. The transparent red lines indicate the accurate map, while the underlying black areas indicate the belief of the robot about the obstacles of the environment.

The left part of figure \ref{fig:loop_closing_comparison} shows the explored map when walking an eight-shaped path through the map without applying loop detection. It can be seen that the belief about the map differs from the actual map in many areas, especially in the lower half of the map because this half has been explored last, exposing it to most of the measurement uncertainties.

The right part of figure \ref{fig:loop_closing_comparison} depicts the exploration when walking the same path as before but applying loop closing. This results in a pose graph with 7 more constrains, providing more information for correcting the poses and corresponding measurements. One can now see that the lower half of the belief about the map is much more accurate since the area in the center has been re-detected by the algorithm.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.50\textwidth]{figures/Loop_closing_compared.jpg}
	\caption{Observed environment without (left) and with (right) loop closing of the pose graph}
	\label{fig:loop_closing_comparison}
\end{figure}

\subsection{Navigation evaluation}
An important aspect which has a high impact on the exploration behavior of the robot is the strategy by which the next navigation goal is determined. When facing the decision for the next goal, the given information are the detected frontiers, which can be spread all over the current map. The most obvious first implementation idea was to select the frontier which lies closest to the robot as the next navigation goal. This is definitely desirable because the robot can then explore this closest point very quickly and find out about the next frontiers.

Figure \ref{fig:navigation_comparison}(a) shows the resulting exploration path of the robot and the explored map after 300 seconds when applying this simple strategy. 10 nodes and 14 constraints have been created in the pose graph during this time which is very little information. Also the nodes are limited a small area of the map. This little degree of exploration is due to the fact that when only considering the closest frontier point there is a high risk that points are selected which are very close to obstacles. When the navigation algorithm tries to find a way to this point, it is inflating the obstacles first, making very close points unreachable. After a point has been discovered to be unreachable, the next closest point to the robot is chosen, which will likely be unreachable as well.

So it can be concluded that the number of close obstacles have to be taken into account in order to select reachable points as next goals. In the upcoming strategy, the sum of obstacles within a radius of 50 grid cells is added to the distance to the robot for every frontier point. The point with the smallest sum is then selected as the next goal. The application of this strategy is shown in figure \ref{fig:navigation_comparison}(b). The robot managed to explore the map within 300 seconds, creating a pose graph with 104 nodes and 150 constraints. However, one can see that the robot did not take a very exploration-efficient route through the map. There are a lot of unreasonable turns in areas that are already explored, while unexplored areas get left out for the time being.

Another aspect is to be added to the evaluation of every frontier point. That is the distance between the currently evaluated frontier point and the previous navigation goal. When this distance is minimized additionally to the distance to the robot and the number of close obstacles, the algorithm will select the next goals in a chain-like fashion, creating a rather intuitive exploration path through the map (figure \ref{fig:navigation_comparison}(c)). The robot was able to explore the map within only 150 seconds, creating a much smaller pose graph of 64 nodes and 66 constraints. This evaluation feature has been given a weight factor of 5 to increase the effect on the goal selection.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.50\textwidth]{figures/Navigation_comparison.jpg}
	\caption{Resulting robot trajectory when applying different strategies for selecting the next navigation goal}
	\label{fig:navigation_comparison}
\end{figure}

\subsection{Runtime analysis}
The implemented Graph SLAM algorithm has been tested on its feasibility. Navigating the robot manually, the complete test map has been explored, building up a pose graph of 123 nodes and 150 constraints. While doing that, the runtime of the complete update function was measured in microseconds. The update function is executed whenever a new node in the graph is created. It contains the algorithms for scan matching and Graph SLAM correction, which have been measured separately.

Figure \ref{fig:runtime} shows the outcome of this runtime measurement. Naturally the runtime increases with increasing graph size, especially due to the growing matrices in the Graph SLAM algorithm.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=0.50\textwidth]{figures/Runtime.png}
	\caption{Runtime of the Graph SLAM update function in micro seconds}
	\label{fig:runtime}
\end{figure}

Even with a graph size of over 100 nodes, the runtime of the function stays in a very feasible range between 60 and 80 milliseconds. The robot can be moved without any delay. Taking only the Graph SLAM update into account, the runtime even stays below 20 milliseconds. In contrast the scan matching is executed outstandingly quickly since it is independent of the size of the pose graph. Most of the time is occupied by updating the occupancy grid because every single grid cell has to redrawn for every pose. In order to compromise this, the update of the complete occupancy grid is only executed every third second and only if a loop closing is detected.
\section{Discussion}
\label{sec:disc}
The Wavefront Frontier detection algorithm \citep{Keidar} succeeds in finding frontiers in a fast and accurate manner.

Implementing the algorithm suggested in \citep{Grisetti} turned out to be well suited for attacking the problem statement of this project. The maps created by this algorithm look accurate. Specifically the margin between the output map and the real map is negligible (FIGURE???). Maps are created in real time and loop closures are added when needed. In fact, the algorithm keeps updating the map with no noticable delay while the robot drives around steering to new goals selected by the WFD algorithm and navigated by ROS' move\_base package.

The second graph-based SLAM implementation \cite{Thrun} clearly is not suited for robots equipped with laser scanners only. Extensive feature extraction methods are required to create occupancy grid maps with the used TurtleBot. Furthermore the algorithm is expected to work best with a robot which is able to detect distinct landmarks as well as a simple environment. A number of unique landmarks can then serve as orientation points to build the map. Additionally the algorithm is able to assess map and trajectory more quickly if there are fewer nodes and constraints in the graph.

\subsubsection{Future research}

\section{Conclusion}
\label{sec:conc}
\bibliography{references}
\nocite{*}
\onecolumn
\appendix

\end{document}
